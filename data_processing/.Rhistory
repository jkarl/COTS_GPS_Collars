path <- "C:/Users/Jason Karl/Documents/GitHub/COTS_GPS_Collars/collar_data"
f <- "001.GPSLOG.CSV"
local.tz <- "America/Denver"
filter.dates <- TRUE
filter.location <- FALSE
date.range <- c("2018-04-27 00:00:00","2018-05-01 23:59:59")
location.range <- c(xmin=-113.453,xmax=-113.433,ymin=42.163,ymax=42.191)
export.points <- TRUE
export.lines <- FALSE
export.dir <- "holding_pasture"
path
export.dir
file.path(path,export.dir)
if (export.points) {
coordinates(filtered.data)<-~Lon+Lat
proj4string(filtered.data)<-CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
writeOGR(filtered.data,file.path(path,export.dir),substr(f,1,nchar(f)-4),driver="ESRI Shapefile",overwrite_layer = TRUE)
}
## Load libraries
library(dplyr)
library(ggplot2)
library(lubridate)
#library(stringr)
library(rgdal)
library(ggmap)
library(geosphere)
library(maptools)
## Set env parameters
path <- "C:/Users/Jason Karl/Documents/GitHub/COTS_GPS_Collars/collar_data"
f <- "001.GPSLOG.CSV"
local.tz <- "America/Denver"
filter.dates <- TRUE
filter.location <- FALSE
date.range <- c("2018-04-27 00:00:00","2018-05-01 23:59:59")
location.range <- c(xmin=-113.453,xmax=-113.433,ymin=42.163,ymax=42.191)
export.points <- TRUE
export.lines <- FALSE
export.dir <- "holding_pasture"
########################################################
## Functions
## Convert UTC times to local
convert_date_time <- function(df, to.local=TRUE) {
parse_date_time(test,"%d%m%y",tz="UTC")
}
## Times less than 10am have only seven digits. Add a leading zero.
fix.time.length <- function(time) {
if (nchar(time)==7){paste("0",time,sep="")} else {time}
}
## Replace dates of 9999 with their antecedant date in the data frame
replace9999 <- function(df, colNum) {
d <- df[,colNum]
d.lag <- lag(d,1)
d.lead <- lead(d,1)
tmp <- cbind(d,d.lag,d.lead)
tmp[tmp[,1]==9999,1]<-(tmp[tmp[,1]==9999,2]+tmp[tmp[,1]==9999,3])/2
df[,colNum]<-tmp
return(df)
}
colNames <- c("HDOP","Lat","Lon","NumSat","UTCDate","UTCTime","FirstFix","LastFix","BestFix")
raw.data <- read.csv(file.path(path,f),col.names=colNames,header=F)
## Count and remove rows with no data
n.missing <- sum(is.na(raw.data$HDOP))
prop.missing <- n.missing/nrow(raw.data)
filtered.data <- raw.data[!is.na(raw.data$HDOP),]
path <- "C:/Users/Jason Karl/Documents/GitHub/COTS_GPS_Collars/2018_collar_data/COTS_Collars"
colNames <- c("HDOP","Lat","Lon","NumSat","UTCDate","UTCTime","FirstFix","LastFix","BestFix")
raw.data <- read.csv(file.path(path,f),col.names=colNames,header=F)
path <- "C:\\Users\\Jason Karl\\Documents\\GitHub\\COTS_GPS_Collars\\COTS_GPS_Collars\\2018_collar_data\\COTS_Collars"
raw.data <- read.csv(file.path(path,f),col.names=colNames,header=F)
n.missing <- sum(is.na(raw.data$HDOP))
prop.missing <- n.missing/nrow(raw.data)
filtered.data <- raw.data[!is.na(raw.data$HDOP),]
n.9999 <- sum(filtered.data$UTCDate==9999)
prop.9999 <- n.missing/nrow(filtered.data)
filtered.data <- filtered.data %>% filter(UTCDate!=9999)
filtered.data <- cbind(filtered.data,"fixedTime"=sapply(filtered.data$UTCTime,fix.time.length))
filtered.data$year <- paste("20",sapply(filtered.data$UTCDate,function(x){substr(x,nchar(x)-1,nchar(x))}),sep="")
filtered.data$month <- sapply(filtered.data$UTCDate,function(x){substr(x,nchar(x)-3,nchar(x)-2)})
filtered.data$day <- sapply(filtered.data$UTCDate,function(x){substr(x,1,nchar(x)-4)})
filtered.data$day <- sapply(filtered.data$day,function(x){if (nchar(x)==1){paste("0",x,sep="")}else{x}}) # Convert all to two-digit day
filtered.data$seconds <- sapply(filtered.data$UTCTime,function(x){substr(x,nchar(x)-3,nchar(x)-2)})
filtered.data$minutes <- sapply(filtered.data$UTCTime,function(x){substr(x,nchar(x)-5,nchar(x)-4)})
filtered.data$hours <- sapply(filtered.data$UTCTime,function(x){substr(x,1,nchar(x)-6)})
filtered.data$hours[filtered.data$hours==''] <- 0 # Trap/fix hours of zero
filtered.data$hours <- sapply(filtered.data$hours,function(x){if (nchar(x)==1){paste("0",x,sep="")}else{x}}) # convert all to two-digit hour
filtered.data$rawDatTime <- paste(paste(filtered.data$day,filtered.data$month,filtered.data$year,sep="/"),paste(filtered.data$hours,filtered.data$minutes,filtered.data$seconds,sep=":"),sep="-")
filtered.data$UTCDateTime <- parse_date_time(filtered.data$rawDatTime,"%d%m%Y-%H%M%S",tz="UTC")
filtered.data$MDTDateTime <- with_tz(filtered.data$UTCDateTime,tzone=local.tz)
## Calculate time difference between subsequent readings
filtered.data$tdiff <- as.numeric(filtered.data$MDTDateTime - lag(filtered.data$MDTDateTime,default=0))
filtered.data$tdiff[1] <- 0 ## Correct the first record.
## Calculate distance between subsequent readings
filtered.data$dist <- mapply(function(Lat1,Lon1,Lat2,Lon2){
distGeo(c(Lon1,Lat1),c(Lon2,Lat2))
},
Lat1=filtered.data$Lat,Lon1=filtered.data$Lon,Lat2=lag(filtered.data$Lat),Lon2=lag(filtered.data$Lon))
filtered.data$dist[1] <- 0 # Correct the first record
## Calculate velocity between subsequent readings
filtered.data$velocity <- filtered.data$dist/filtered.data$tdiff
## Apply location and date filters
if (filter.dates==TRUE){
filtered.data <- filtered.data %>% filter(MDTDateTime>=date.range[1] & MDTDateTime<=date.range[2])
}
if (filter.location==TRUE){
filtered.data <- filtered.data %>% filter(Lon>=location.range['xmin'] & Lon<=location.range['xmax'] & Lat>=location.range['ymin'] & Lat<=location.range['ymax'])
}
## Finally, add an ID field
filtered.data$id <- as.integer(rownames(filtered.data))
if (export.lines) {
to.lines <- data.frame(Lat2=filtered.data$Lat,Lon2=filtered.data$Lon,Lat1=lag(filtered.data$Lat,default=filtered.data$Lat[1]),Lon1=lag(filtered.data$Lon,default=filtered.data$Lon[1]),id=filtered.data$id-1)
lines <- mapply(function(Lat1,Lon1,Lat2,Lon2,id){
Lines(Line(cbind(c(Lon1,Lon2),c(Lat1,Lat2))),ID=id)
},
to.lines$Lat1,to.lines$Lon1,to.lines$Lat2,to.lines$Lon2,to.lines$id)
spatial.lines <- SpatialLinesDataFrame(SpatialLines(lines),data=to.lines,match.ID = FALSE)
proj4string(spatial.lines)<-CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
writeOGR(spatial.lines,file.path(path,export.dir),paste(substr(f,1,nchar(f)-4),"lines",sep="_"),driver="ESRI Shapefile",overwrite_layer = TRUE)
}
if (export.points) {
coordinates(filtered.data)<-~Lon+Lat
proj4string(filtered.data)<-CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
writeOGR(filtered.data,file.path(path,export.dir),substr(f,1,nchar(f)-4),driver="ESRI Shapefile",overwrite_layer = TRUE)
}
bbox <- c(left=filtered.data@bbox[1,1],bottom=filtered.data@bbox[2,1],right=filtered.data@bbox[1,2],top=filtered.data@bbox[2,2])
m <- get_map(location=bbox,maptype="hybrid",zoom=14)
ggmap(m)+geom_point(data=data.frame(filtered.data), aes(x=Lon,y=Lat,color=MDTDateTime))
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
source('~/GitHub/COTS_GPS_Collars/COTS_GPS_Collars/data_processing/process_COTS_collar_data.R', echo=TRUE)
library(rgdal)
library(dplyr)
library(geosphere)
library(lubridate)
library(ggplot2)
library(ggmap)
library(maptools)
## Load the iGotU CSV file
##### Note: iGotU uses a UTF-16 encoding. Failure to specify that encoding will result in errors and blank cells on import
raw.data <- read.csv(file.path(params$path,params$file),header=T,stringsAsFactors=F,fileEncoding = "UTF-16LE")
#make sure lat/lon fields are numeric
raw.data$Latitude <- as.numeric(raw.data$Latitude)
raw.data$Longitude <- as.numeric(raw.data$Longitude)
## Remove records with Lat/Lon equal to zero
filtered.data <- raw.data %>% filter(Latitude != 0 & Longitude != 0)
## Format Date/Time field
filtered.data$MDTDateTime <- parse_date_time(paste(filtered.data$Date,filtered.data$Time,sep="-"),"%Y/%m%d-%H:%M:%S",tz=params$local.tz)
## Apply location and date filters
if (params$filter.dates==TRUE){
date.range <- eval(parse(text=params$date.range))
filtered.data <- filtered.data %>% filter(MDTDateTime>=date.range[1] & MDTDateTime<=date.range[2])
}
if (params$filter.location==TRUE){
location.range <- eval(parse(text=params$location.range))
filtered.data <- filtered.data %>% filter(Longitude>=location.range['xmin'] & Longitude<=location.range['xmax'] & Latitude>=location.range['ymin'] & Latitude<=location.range['ymax'])
}
## Calculate time difference between subsequent readings
filtered.data$tdiff <- as.numeric(filtered.data$MDTDateTime - lag(filtered.data$MDTDateTime,default=0))
filtered.data$tdiff[1] <- 0 ## Correct the first record.
## Calculate distance between subsequent readings
filtered.data$dist <- mapply(function(Lat1,Lon1,Lat2,Lon2){
distGeo(c(Lon1,Lat1),c(Lon2,Lat2))
},
Lat1=filtered.data$Latitude,Lon1=filtered.data$Longitude,Lat2=lag(filtered.data$Latitude),Lon2=lag(filtered.data$Longitude))
filtered.data$dist[1] <- 0 # Correct the first record
## Calculate velocity between subsequent readings
filtered.data$velocity <- filtered.data$dist/filtered.data$tdiff
## export result as shapefiles for lines and points
to.lines <- data.frame(Lat2=filtered.data$Latitude,Lon2=filtered.data$Longitude,Lat1=lag(filtered.data$Latitude,default=filtered.data$Latitude[1]),Lon1=lag(filtered.data$Longitude,default=filtered.data$Longitude[1]),id=filtered.data$Index-1)
coordinates(filtered.data)<-~Longitude+Latitude
proj4string(filtered.data)<-CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
writeOGR(filtered.data,params$path,substr(params$file,1,nchar(params$file)-4),driver="ESRI Shapefile",overwrite_layer = TRUE)
library(ggplot2)
infile <- "C:\\Users\\Jason Karl\\Dropbox (IFIRE)\\Rangelands\\Mosley_Figs\\Fig1.csv"
data <- read.csv(infile, header=T, stringsAsFactors = F)
names(data) <- c("Year","numBison","numElk","Label")
data$Label[data$Label==NA] <- " "
f1 <- ggplot(data=data,aes(x=as.Date(paste(Year,7,1,sep="-")),y=numBison))+
geom_bar(stat="identity", fill="#3182bd")+
ylab("Number of Bison x 10")+xlab("Year")+
scale_x_date(limits = as.Date(c("1978-1-1","2019-1-1")),
date_breaks = "2 years",
date_labels = "%Y",
expand=(c(0,0)))+
theme(axis.text.x = element_text(angle=90))
f1
ggsave("C:\\Users\\Jason Karl\\Dropbox (IFIRE)\\Rangelands\\Mosley_Figs\\Fig1_corrected.tif",f1,device="tiff",dpi=300,width=6,units="in")
f2 <- ggplot(data=data,aes(x=as.Date(paste(Year,7,1,sep="-")),y=numElk))+
geom_bar(stat="identity", fill="#3182bd")+
ylab("Number of Elk x 10")+xlab("Year")+
scale_x_date(limits = as.Date(c("1978-1-1","2019-1-1")),
date_breaks = "2 years",
date_labels = "%Y",
expand=(c(0,0)))+
theme(axis.text.x = element_text(angle=90))
f2
ggsave("C:\\Users\\Jason Karl\\Dropbox (IFIRE)\\Rangelands\\Mosley_Figs\\Fig2_corrected.tif",f2,device="tiff",dpi=300,width=6,units="in")
library(ggplot2)
infile <- "C:\\Users\\Jason Karl\\Dropbox (IFIRE)\\Rangelands\\Mosley_Figs\\Fig1.csv"
data <- read.csv(infile, header=T, stringsAsFactors = F)
names(data) <- c("Year","numBison","numElk","Label")
data$Label[data$Label==NA] <- " "
f1 <- ggplot(data=data,aes(x=as.Date(paste(Year,7,1,sep="-")),y=numBison))+
geom_bar(stat="identity", fill="#3182bd")+
ylab("Number of Bison x 10")+xlab("Year")+
scale_x_date(limits = as.Date(c("1978-1-1","2019-1-1")),
date_breaks = "2 years",
date_labels = "%Y",
expand=(c(0,0)))+
theme(axis.text.x = element_text(angle=90))
f1
f1 <- ggplot(data=data,aes(x=as.Date(paste(Year,7,1,sep="-")),y=numBison))+
geom_bar(stat="identity", fill="#3182bd")+
ylab("Number of Bison x 10")+xlab("Year")+
scale_x_date(limits = as.Date(c("1978-1-1","2019-1-1")),
date_breaks = "2 years",
date_labels = "%Y",
expand=(c(0,0)))+
theme(axis.text.x = element_text(angle=90))
f1
